{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35f134-7646-4bcf-abf5-342a16108563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f24466-68b2-4006-8778-68b0b675da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13db73-1a08-4ef0-98e8-5531a4d2581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration, T5Config\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cadf40-d0e6-47bc-bb0f-6a93aba6ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding='do_not_pad',\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd26d2a-7cec-4afd-9c87-811632b02f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c635d-ec87-4502-a0eb-96ac726d2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd020d-2215-462d-a80f-7aefd65fec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class QuestionAnswering(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        # model: T5 with encoder and decoder\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.qa_outputs = nn.Linear(model.config.hidden_size, 2)\n",
    "        \n",
    "\n",
    "    def question_answering(self, batch):\n",
    "        \"\"\"\n",
    "        For each token in sequence predict its probability of\n",
    "        being starting position and ending position\n",
    "        \"\"\"\n",
    "        outputs = self.model.encoder(\n",
    "            batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        total_loss = None\n",
    "        \n",
    "        # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "        ignored_index = start_logits.size(1)\n",
    "        start_positions = batch['start_positions'].clamp(0, ignored_index)\n",
    "        end_positions = batch['end_positions'].clamp(0, ignored_index)\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "        start_loss = loss_fct(start_logits, start_positions)\n",
    "        end_loss = loss_fct(end_logits, end_positions)\n",
    "        total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        output = {\n",
    "            'start_logits': start_logits.detach().cpu(),\n",
    "            'end_logits': end_logits.detach().cpu(),\n",
    "            'start_positions': start_positions.cpu(),\n",
    "            'end_positions': end_positions.cpu(),\n",
    "        }\n",
    "        return total_loss, output\n",
    "    \n",
    "    def em(self, predicted: Tuple[int, int], ground_truth: Tuple[int, int]):\n",
    "        \"\"\"\n",
    "        calculates exact match metric\n",
    "        \"\"\"\n",
    "        return list(predicted) == list(ground_truth)\n",
    "    \n",
    "    def f1(self, predicted: Tuple[int, int], ground_truth: Tuple[int, int]):\n",
    "        \"\"\"\n",
    "        calculates f1 metric\n",
    "        \"\"\"\n",
    "\n",
    "        if predicted[1] < predicted[0]:\n",
    "            return 0\n",
    "        \n",
    "        predicted = set(range(predicted[0], predicted[1] + 1))\n",
    "        ground_truth = set(range(ground_truth[0], ground_truth[1] + 1))\n",
    "        \n",
    "        tp = len(predicted & ground_truth)\n",
    "        if tp == 0:\n",
    "            return 0\n",
    "\n",
    "        fp = len(predicted - ground_truth)\n",
    "        fn = len(ground_truth - predicted)\n",
    "        \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        \n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    def train_one_epoch(self, dataloader, optimizer):\n",
    "        self.train()\n",
    "        \n",
    "        ems = []\n",
    "        f1s = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                loss, output = self.question_answering(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            predicted = torch.stack([\n",
    "                output['start_logits'].argmax(-1),\n",
    "                output['end_logits'].argmax(-1)\n",
    "            ]).T\n",
    "            ground_truth = torch.stack([\n",
    "                output['start_positions'],\n",
    "                output['end_positions']\n",
    "            ]).T\n",
    "            \n",
    "            ems.extend([self.em(pred, truth) for pred, truth in zip(predicted, ground_truth)])\n",
    "            f1s.extend([self.f1(pred, truth) for pred, truth in zip(predicted, ground_truth)])\n",
    "            \n",
    "            wandb.log({\n",
    "                'em': np.mean(ems[-32:]),\n",
    "                'f1': np.mean(f1s[-32:])\n",
    "            })\n",
    "\n",
    "        return np.mean(ems), np.mean(f1s)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        self.eval()\n",
    "        \n",
    "        ems = []\n",
    "        f1s = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "            loss, output = self.question_answering(batch)\n",
    "            \n",
    "            predicted = torch.stack([\n",
    "                output['start_logits'].argmax(-1),\n",
    "                output['end_logits'].argmax(-1)\n",
    "            ]).T\n",
    "            ground_truth = torch.stack([\n",
    "                output['start_positions'],\n",
    "                output['end_positions']\n",
    "            ]).T\n",
    "            \n",
    "            ems.extend([self.em(pred, truth) for pred, truth in zip(predicted, ground_truth)])\n",
    "            f1s.extend([self.f1(pred, truth) for pred, truth in zip(predicted, ground_truth)])\n",
    "\n",
    "        return np.mean(ems), np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ea180-09fb-408c-bbe8-619a466c822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def collate_batch(pad_id, batch):\n",
    "    input_ids = []\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for sample in batch:\n",
    "        input_ids.append(torch.tensor(sample['input_ids'], dtype=torch.long))\n",
    "        start_positions.append(sample['start_positions'])\n",
    "        end_positions.append(sample['start_positions'])\n",
    "\n",
    "    batch = {\n",
    "        'input_ids': pad_sequence(input_ids, padding_value=pad_id, batch_first=True),\n",
    "        'start_positions': torch.tensor(start_positions, dtype=torch.long),\n",
    "        'end_positions': torch.tensor(start_positions, dtype=torch.long)\n",
    "    }\n",
    "    batch['attention_mask'] = (batch['input_ids'] != pad_id).clone()\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "qa_train_loader = torch.utils.data.DataLoader(\n",
    "    tokenized_squad['train'],\n",
    "    collate_fn=partial(collate_batch, tokenizer.pad_token_id),\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "qa_val_loader = torch.utils.data.DataLoader(\n",
    "    tokenized_squad['validation'],\n",
    "    collate_fn=partial(collate_batch, tokenizer.pad_token_id),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf177d6-9e89-4941-9d6e-84615facc02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration(T5Config.from_pretrained('t5-small'))\n",
    "model.load_state_dict(torch.load('your/pretrained/model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b163a58-651e-4d57-939c-9d41212caa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = QuestionAnswering(model).to(device)\n",
    "optimizer = torch.optim.AdamW(qa.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867d6fe-ff7a-4ed0-a3ad-3f080c01dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='project', name='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af5714-6403-48fa-87cd-cc3374f56115",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    qa.train_one_epoch(qa_train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1e1d5-e9e8-406b-b8f9-89785196595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.evaluate(qa_train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
