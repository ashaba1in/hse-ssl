{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a3d4e4-6a02-4ce1-a0d1-08b78c0e21f2",
   "metadata": {},
   "source": [
    "# Методы предобучения без учителя, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 3: предобучение языковых моделей\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — **13** баллов. Сдавать задание после указанного срока сдачи нельзя.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a1114-d33c-470a-bfc9-466c8493f5eb",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В этом задании мы реализуем три (четыре) метода предобучения языковых моделей и сравним их качество на трех downstream задачах. Мы будем работать с моделью T5-small, имеющую архитектуру энкодер-декодер. Для предобучения будем использовать датасет [BookCorpus](https://huggingface.co/datasets/bookcorpus) (точнее его десятую часть, чтобы было реально обучиться за ограниченное время). Этот датасет содержит 74 миллиона предложений из книг и использовался для предобучения практически всех моделей, так что нам он идеально подойдет.  \n",
    "\n",
    "Как и в прошлой домашке, для сдачи задания необходимо провести эксперименты, описанные в ноутбуке, и написать о них отчет в формате PDF. Вместе с отчетом сдается код, позволяющий запустить эксперименты. Прежде чем что-то имплементировать, прочитайте все постановки экспериментов и подумайте, как лучше организовать код, не забудьте также зачекпойнтить необходимые обученные модели. Мы оставляем за собой право снижать оценку за плохо структурированный код. **Обязательно** используйте оптимизации для обучения из [списка](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html) в своих пайплайнах, чтобы ускорить запуски. Особенно Automatic Mixed Precision. Также **обратите внимание**, что отчет &mdash; это обязательная составляющая оценки, без него мы не будем проверять ваше задание. Отчет обязательно должен включать кривые обучения для всех моделей, которые вы запускаете. Кроме того, следите за тем, чтобы ваши рисунки и графики были читаемыми, и по возможности красивыми :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54bdfea-71fb-4364-b475-a504f3ef34aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "При тестировании наших методов предобучения мы будем проводить замеры, дообучая модель на трех задачах по отдельности:\n",
    "\n",
    "* Ответы на вопросы: датасет [SQuAD](https://huggingface.co/datasets/squad), метрики **ExactMatch (EM)** и **F1**.\n",
    "* Суммаризация: датасет [XSum](https://huggingface.co/datasets/xsum), метрика **ROUGE-1**.\n",
    "* Машинный перевод: **сотая часть** датасета [WMT-16 [de-en]](https://huggingface.co/datasets/wmt16), метрика **BLEU**.\n",
    "\n",
    "Код для дообучения вы можете написать сами, а можете ориентироваться на ноутбуки, прикрепленные к заданию. Так же можно использовать официальные туториалы от huggingface ([ответы на вопросы](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb), [суммаризация](https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb), [перевод](https://github.com/huggingface/notebooks/blob/main/examples/translation.ipynb)). Туториалы реализованы через специализированные классы, что удобно, однако работают они существенно медленнее.\n",
    "\n",
    "\n",
    "## Задание 0. Supervised baseline\n",
    "\n",
    "**0 баллов, но при невыполнении максимум за все задание &mdash; 0 баллов**\n",
    "\n",
    "Как обычно, перед исследованием задач предобучения построим бейзлайн с помощью модели, обученной supervised из случайного начального приближения. Для этого задания и всех остальных далее зафиксируем архитектуру T5-small. Гиперпараметры для обучения разрешается выбрать любыми разумными, главное --сохраняйте их неизменными для всех заданий, чтобы сравнение было валидным. **Обратите внимание**, что в задаче ответов на вопросы от модели требуется только энкодер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aab2bb-e7ed-4651-ac7e-5cd94d8cbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Config\n",
    "model = T5ForConditionalGeneration(T5Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4930b60-863a-4793-93b1-397669fc10a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45677f17-808e-48fe-8b4b-f9bca26798cd",
   "metadata": {},
   "source": [
    "## Задание 1. MLM\n",
    "\n",
    "**3 балла**\n",
    "\n",
    "Реализуйте и обучите метод Masked Language Model (MLM) из статьи [Devlin et al, 2018](https://arxiv.org/pdf/1810.04805.pdf). В этом задании запрещается заимствовать код из открытых источников. Параметры для маскирования следует взять такими же, как в статье. Для обучения используйте **десятую часть** датасета BookCorpus. Длину всех текстов ограничим значением 256. Обучение скорее всего будет занимать больше часа на одну эпоху, поэтому для экспериментов советуем взять небольшую подвыборку. При обучении на всей выборке 2-3 эпох должно быть достаточно, чтобы получить улучшение над supervised моделью.\n",
    "\n",
    "При дообучении на seq2seq задачи декодер придется инициализировать случайно.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121cb304-15df-49fb-9ef9-9745044afc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bookcorpus\", split=\"train\", num_proc=8)\n",
    "dataset = dataset.select(range(int(len(dataset) * 0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6af39-eb98-4f79-82a3-f537d8c00f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852092c5-5d0c-44c9-8e16-63812cf46fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(lambda examples: tokenizer(examples['text'], max_length=256, batched=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37676a5f-8972-48a9-91c9-5d8f6772f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.save_to_disk(\"encoded_bookcorpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bc273",
   "metadata": {},
   "source": [
    "При создании батчей очень важно тратить как можно меньше места на паддинги, при этом полностью заполняя память. Для этого размер батча удобнее измерять числом токенов, а не количеством текстов. Тогда в каждом батче будут лежать тексты примерно одной длины так, чтобы число токенов, включая паддинги, было примерно фиксированным. К сожалению, хорошей реализации такого семплирования я никогда не видел, поэтому ниже мой вариант."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Sampler\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "class FixedTokensSampler(Sampler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: datasets.arrow_dataset.Dataset,\n",
    "        n_tokens: int,\n",
    "        lengths: np.array = None,\n",
    "        max_len: int = 256,\n",
    "        shuffle: bool = True\n",
    "    ):\n",
    "        self.n_tokens = n_tokens\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if lengths is not None:\n",
    "            self.lengths = lengths\n",
    "        else:\n",
    "            # default iteration over a dataset is too slow\n",
    "            self.lengths = np.array([(i, len(sample)) for i, sample in enumerate(tqdm(dataset._data['input_ids']))])\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            # fast shuffle\n",
    "            self.lengths = self.lengths[np.random.permutation(len(self.lengths))]\n",
    "\n",
    "        mean_length = np.mean(self.lengths, axis=0)[1]\n",
    "        \n",
    "        # approximatelly 100 batches of with mean-sized samples\n",
    "        step = int(self.n_tokens / mean_length * 100)\n",
    "        for i in range(0, len(self.lengths), step):\n",
    "            pooled = sorted(self.lengths[i:i + step], key=lambda x: x[1])\n",
    "\n",
    "            batches = []\n",
    "            batch = []\n",
    "            cur_n_tokens = 0\n",
    "            for idx, length in pooled:\n",
    "                # lengths are sorted in ascending order\n",
    "                if (len(batch) + 1) * length > self.n_tokens:\n",
    "                    if len(batch) == 0:\n",
    "                        print(f'Maximum number of tokens {self.n_tokens} is lower, than size of a sample {length}')\n",
    "                        break\n",
    "                    else:\n",
    "                        batches.append(copy(batch))\n",
    "                        batch = []\n",
    "                        cur_n_tokens = 0\n",
    "                else:\n",
    "                    batch.append(idx)\n",
    "                    cur_n_tokens += length\n",
    "            \n",
    "            if self.shuffle:\n",
    "                random.shuffle(batches)\n",
    "\n",
    "            for batch in batches:\n",
    "                yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        # approximate size (lower bound)\n",
    "        return np.sum(self.lengths, axis=0)[1] // self.n_tokens\n",
    "\n",
    "\n",
    "def collate_batch(batch, pad_id=0):\n",
    "    \"\"\"\n",
    "    this function recieves batch and returns its modified version\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    for sample in batch:\n",
    "        input_ids.append(torch.tensor(sample['input_ids'])) \n",
    "\n",
    "    return pad_sequence(input_ids, padding_value=pad_id, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3acf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = FixedTokensSampler(encoded_dataset, n_tokens=..., shuffle=True)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    encoded_dataset,\n",
    "    batch_sampler=sampler,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e879d-44f6-4637-9365-01a18ab3cd8a",
   "metadata": {},
   "source": [
    "## Задание 2. GPT\n",
    "\n",
    "**3 балла**\n",
    "\n",
    "Аналогично предыдущему заданию, реализуйте language modeling задачу предобучения для декодера как это было сделано в статье про GPT ([Radford et al, 2018](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)). Как изменилось качество на downstream задачах относительно MLM? Почему так?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c5129-7810-41da-ac37-7a4e5289574e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fd4cbc4-7e6a-4d1b-b029-e989bb5dbbd0",
   "metadata": {},
   "source": [
    "## Задание 3. MASS\n",
    "\n",
    "**4 балла**\n",
    "\n",
    "Реализуйте метод MASS ([Song et al, 2019](https://arxiv.org/pdf/1905.02450.pdf)) для предобучение энкодера и декодера с помощью демаскирования последовательностей токенов. В оригинальной статье предлагается маскировать только одну последовательность, однако в подходе T5 ([Raffel et al, 2020](https://arxiv.org/pdf/1910.10683.pdf)) предлагается маскировать несколько последовательностей небольшой длины. Оба способа должны показать прирост в метриках при дообучении на seq2seq задачах. Проверьте, какой способ лучше и прокомментируйте результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c655211-0409-4586-88ca-7d911ab41dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45cb969a-ad5f-4c72-95d8-1f8d2b56acad",
   "metadata": {},
   "source": [
    "## Бонус 1. BART\n",
    "\n",
    "**3 балла**\n",
    "\n",
    "Аналогично предыдущим заданиям реализуйте метод BART ([Lewis et al, 2019](https://arxiv.org/pdf/1910.13461.pdf)) и замерьте качество. Не обязательно брать все 5 функциналов для обучения, трех на ваш выбор будет достаточно.\n",
    "\n",
    "В статье предлагается использовать случайно инициализированный энкодер для задачи машинного перевода, чтобы работать с языками, отличными от предобучающих. Проверьте на нашем датасете для перевода, есть ли в этом смысл, обучив модель переводить с немецкого на английский."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b2812-3f12-4fd3-8a0c-386c3c8fc983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
