{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eaefb5-9468-4c9a-b00b-a17ead2b39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ddaaee-7346-472c-8030-fa4f70ad1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567f981-3607-49d7-847e-bb19922dc595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\")\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8e114-dda9-4029-98a6-44f291212de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration, T5Config\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0809b1b-f1db-41d0-95af-4daaa8deba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"document\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252255f-7c61-4606-9be6-598b8d07ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d9506-8c8f-4e09-8951-990d1f04e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarization(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        # model: T5 with encoder and decoder\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "\n",
    "    def summarize(self, batch):\n",
    "        outputs = self.model(\n",
    "            batch['input_ids'],\n",
    "            labels=batch['labels'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "\n",
    "        # outputs = model(input_ids=input_ids, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train_one_epoch(self, dataloader, optimizer):\n",
    "        self.train()\n",
    "        \n",
    "        for batch in tqdm(dataloader):\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                loss = self.summarize(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            wandb.log({\n",
    "                'loss': loss.item()\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee30a9-d6db-4f11-b445-87e19dce9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def collate_batch(pad_id, batch):\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    for sample in batch:\n",
    "        input_ids.append(torch.tensor(sample['input_ids'], dtype=torch.long))\n",
    "        labels.append(torch.tensor(sample['labels'], dtype=torch.long))\n",
    "\n",
    "    batch = {\n",
    "        'input_ids': pad_sequence(input_ids, padding_value=pad_id, batch_first=True),\n",
    "        'labels': pad_sequence(labels, padding_value=-100, batch_first=True)\n",
    "    }\n",
    "    batch['attention_mask'] = (batch['input_ids'] != pad_id).clone()\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "sum_train_loader = torch.utils.data.DataLoader(\n",
    "    tokenized_datasets['train'],\n",
    "    collate_fn=partial(collate_batch, tokenizer.pad_token_id),\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "sum_val_loader = torch.utils.data.DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    collate_fn=partial(collate_batch, tokenizer.pad_token_id),\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fe222-6fe8-49a7-b394-12a5ac449177",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration(T5Config.from_pretrained('t5-small'))\n",
    "model.load_state_dict(torch.load('your/pretrained/model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69cb60-11ae-49e3-9480-2c6c2f6449c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization = Summarization(model).to(device)\n",
    "optimizer = torch.optim.AdamW(summarization.parameters(), lr=2e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2282d6b-6a89-4c8f-88b1-4e6292008236",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='project', name='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a38943-05f8-473e-9329-be6f134505be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    summarization.train_one_epoch(sum_train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be889143-ad3e-40bd-b473-17314949449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    rouges = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output_sequences = model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                do_sample=False,  # disable sampling to test if batching affects output\n",
    "            )\n",
    "\n",
    "        rouge = compute_metrics((output_sequences.cpu(), batch['labels'].cpu()))['rouge1']\n",
    "        rouges.append(rouge)\n",
    "\n",
    "    return np.mean(rouges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57210ee-0d9c-4d59-8ae9-6523cb85fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(summarization.model, sum_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7787bae-42b6-42f5-ae95-8e720454715d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
